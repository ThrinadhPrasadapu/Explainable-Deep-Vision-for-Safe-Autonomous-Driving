{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7648024,"sourceType":"datasetVersion","datasetId":4458183}],"dockerImageVersionId":31192,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nimport numpy as np\nimport cv2\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom torchvision.models import resnet18, ResNet18_Weights","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-04T18:07:26.438558Z","iopub.execute_input":"2025-12-04T18:07:26.438866Z","iopub.status.idle":"2025-12-04T18:07:26.445212Z","shell.execute_reply.started":"2025-12-04T18:07:26.438844Z","shell.execute_reply":"2025-12-04T18:07:26.444235Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"\n\n# ---------------------------\n# Config\n# ---------------------------\nclass Config:\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    img_size = 320\n    batch_size = 16\n    lr = 1e-4\n    epochs = 10\n    num_workers = 4\n    num_classes = 3  # no fog, medium fog, dense fog\n    SAVE_DIR = './checkpoints'\n    classes = ['No_Fog', 'Medium_Fog', 'Dense_Fog']  # Match your folder names exactly\n\nos.makedirs(Config.SAVE_DIR, exist_ok=True)\n\n# ---------------------------\n# Grad-CAM for Classification\n# ---------------------------\nclass GradCAM:\n    def __init__(self, model, target_layer):\n        self.model = model\n        self.target_layer = target_layer\n        self.gradients = None\n        self.activations = None\n        self.hook_handle = None\n\n        def save_activation(module, input, output):\n            self.activations = output.detach()\n\n        def save_gradient(grad):\n            self.gradients = grad\n\n        self.hook_handle = self.target_layer.register_forward_hook(save_activation)\n        self.target_layer.register_backward_hook(lambda module, grad_in, grad_out: save_gradient(grad_out[0]))\n\n    # def __call__(self, x, class_idx=None):\n    #     self.model.zero_grad()\n    #     output = self.model(x)\n    #     if class_idx is None:\n    #         class_idx = output.argmax(dim=1).item()\n    #     one_hot = torch.zeros_like(output)\n    #     one_hot[0][class_idx] = 1\n    #     output.backward(gradient=one_hot, retain_graph=False)\n\n    #     weights = torch.mean(self.gradients, dim=(2, 3), keepdim=True)\n    #     cam = torch.sum(weights * self.activations, dim=1, keepdim=True)\n    #     cam = F.relu(cam)\n    #     cam = F.interpolate(cam, size=(x.shape[2], x.shape[3]), mode='bilinear', align_corners=False)\n    #     cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n    #     return cam[0, 0].cpu().numpy()\n    def __call__(self, x, class_idx=None):\n        self.model.zero_grad()\n        model_output = self.model(x)\n        \n        # Handle tuple output (logits, features)\n        if isinstance(model_output, tuple):\n            logits = model_output[0]\n        else:\n            logits = model_output\n\n        if class_idx is None:\n            class_idx = logits.argmax(dim=1).item()\n        \n        one_hot = torch.zeros_like(logits)\n        one_hot[0][class_idx] = 1\n        logits.backward(gradient=one_hot, retain_graph=False)\n\n        # Compute weights via global average pooling of gradients\n        weights = torch.mean(self.gradients, dim=(2, 3), keepdim=True)  # [1, C, 1, 1]\n        cam = torch.sum(weights * self.activations, dim=1, keepdim=True)  # [1, 1, H, W]\n        cam = F.relu(cam)\n        cam = F.interpolate(cam, size=(x.shape[2], x.shape[3]), mode='bilinear', align_corners=False)\n        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n        return cam[0, 0].cpu().numpy()\n    def remove_hook(self):\n        if self.hook_handle:\n            self.hook_handle.remove()\n\n# ---------------------------\n# Dataset for Fog Classification (Accepts list of samples)\n# ---------------------------\nclass FogClassificationDataset(Dataset):\n    def __init__(self, samples, transform=None):\n        \"\"\"\n        samples: list of (image_path, label)\n        \"\"\"\n        self.samples = samples\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        img_path, label = self.samples[idx]\n        img = cv2.imread(img_path)\n        if img is None:\n            raise ValueError(f\"Failed to load image: {img_path}\")\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        if self.transform:\n            img = self.transform(img)\n        return img, label, img_path\n\n# ---------------------------\n# Improved LTC Cell (Kept as-is)\n# ---------------------------\nclass ImprovedLTCCell(nn.Module):\n    def __init__(self, input_dim, hidden_dim, time_steps=10):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.time_steps = time_steps\n        self.W_ih = nn.Linear(input_dim, hidden_dim)\n        self.W_hh = nn.Linear(hidden_dim, hidden_dim)\n        self.tau = nn.Parameter(torch.ones(hidden_dim) * 0.5)\n        self.alpha = nn.Parameter(torch.ones(hidden_dim))\n        self.beta = nn.Parameter(torch.zeros(hidden_dim))\n\n    def forward(self, x_seq, hidden=None):\n        T, B, _ = x_seq.shape\n        if hidden is None:\n            hidden = torch.zeros(B, self.hidden_dim, device=x_seq.device)\n        tau = F.softplus(self.tau) + 0.01\n        dt = 1.0 / self.time_steps\n        outputs = []\n        for t in range(T):\n            inp = x_seq[t]\n            for _ in range(self.time_steps):\n                activation = torch.tanh(self.alpha * (self.W_ih(inp) + self.W_hh(hidden)) + self.beta)\n                dhdt = (-hidden + activation) / tau.unsqueeze(0)\n                hidden = hidden + dt * dhdt\n            outputs.append(hidden)\n        return torch.stack(outputs, dim=0), hidden\n\nclass LiquidTemporalModule(nn.Module):\n    def __init__(self, feat_dim, hidden_dim=256, num_layers=2):\n        super().__init__()\n        self.proj = nn.Linear(feat_dim, hidden_dim)\n        self.ltc_layers = nn.ModuleList([\n            ImprovedLTCCell(hidden_dim, hidden_dim) for _ in range(num_layers)\n        ])\n\n    def forward(self, feat_seq):\n        if not feat_seq:\n            return torch.zeros(1, 256, device=feat_seq[0].device)\n        seq = torch.stack([self.proj(f) for f in feat_seq], dim=0)\n        hidden = None\n        for ltc in self.ltc_layers:\n            seq, hidden = ltc(seq, hidden)\n        return seq[-1]\n\n# ---------------------------\n# Final Model: Fog Classifier with LTC & Dehaze\n# ---------------------------\nclass FogClassifier(nn.Module):\n    def __init__(self, num_classes=3, use_ltc=True):\n        super().__init__()\n        self.use_ltc = use_ltc\n        self.dehaze = nn.Sequential(\n            nn.Conv2d(3, 16, 3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(16, 3, 1),\n            nn.Sigmoid()\n        )\n        # Load ImageNet weights only if available locally ‚Äì no download attempt\n        self.backbone = resnet18(weights=None)  # Or weights=ResNet18_Weights.DEFAULT if cached\n        self.backbone.fc = nn.Identity()\n        feat_dim = 512\n\n        if use_ltc:\n            self.temporal = LiquidTemporalModule(feat_dim, hidden_dim=256, num_layers=2)\n            self.classifier = nn.Linear(256, num_classes)\n        else:\n            self.classifier = nn.Linear(feat_dim, num_classes)\n\n        self.backbone_layer4 = self.backbone.layer4\n\n    def forward(self, x):\n        if x.dim() == 5:\n            x = x.squeeze(1)\n        x = self.dehaze(x)\n        features = self.backbone.conv1(x)\n        features = self.backbone.bn1(features)\n        features = self.backbone.relu(features)\n        features = self.backbone.maxpool(features)\n        features = self.backbone.layer1(features)\n        features = self.backbone.layer2(features)\n        features = self.backbone.layer3(features)\n        features = self.backbone.layer4(features)\n\n        pooled = F.adaptive_avg_pool2d(features, (1,1)).view(x.size(0), -1)\n        if self.use_ltc:\n            latent = self.temporal([pooled])\n        else:\n            latent = pooled\n        logits = self.classifier(latent)\n        return logits, features\n\n# ---------------------------\n# Training Utilities\n# ---------------------------\ndef train_one_epoch(model, dataloader, optimizer, device):\n    model.train()\n    total_loss = 0.0\n    correct = 0\n    total = 0\n    pbar = tqdm(dataloader, desc=\"Training\")\n    for imgs, labels, _ in pbar:\n        imgs, labels = imgs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        logits, _ = model(imgs.unsqueeze(1))\n        loss = F.cross_entropy(logits, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n        preds = logits.argmax(dim=1)\n        correct += preds.eq(labels).sum().item()\n        total += labels.size(0)\n        pbar.set_postfix({\"Loss\": loss.item(), \"Acc\": correct/total})\n    return total_loss / len(dataloader), correct / total\n\ndef validate(model, dataloader, device):\n    model.eval()\n    total_loss = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for imgs, labels, _ in dataloader:\n            imgs, labels = imgs.to(device), labels.to(device)\n            logits, _ = model(imgs.unsqueeze(1))\n            loss = F.cross_entropy(logits, labels)\n            total_loss += loss.item()\n            preds = logits.argmax(dim=1)\n            correct += preds.eq(labels).sum().item()\n            total += labels.size(0)\n    return total_loss / len(dataloader), correct / total","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T18:07:26.455684Z","iopub.execute_input":"2025-12-04T18:07:26.456003Z","iopub.status.idle":"2025-12-04T18:07:26.487673Z","shell.execute_reply.started":"2025-12-04T18:07:26.455979Z","shell.execute_reply":"2025-12-04T18:07:26.486597Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# ---------------------------\n# Visualization with Grad-CAM ‚Äî One per class\n# ---------------------------\ndef visualize_gradcam_per_class(model, dataloader, device, gradcam, class_names):\n    model.eval()\n    # Collect one example per class\n    examples = {0: None, 1: None, 2: None}\n\n    for imgs, labels, paths in dataloader:\n        for i in range(len(labels)):\n            label = labels[i].item()\n            if examples[label] is None:\n                examples[label] = (imgs[i], label)\n            if all(v is not None for v in examples.values()):\n                break\n        if all(v is not None for v in examples.values()):\n            break\n\n    # Plot one per class\n    fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n    fig.suptitle(\"Grad-CAM: One Sample Per Fog Class\", fontsize=16)\n\n    for row, (label, (img_tensor, true_label)) in enumerate(examples.items()):\n        img_tensor = img_tensor.unsqueeze(0).to(device)  # [1, C, H, W]\n        img_np = img_tensor[0].cpu().numpy().transpose(1, 2, 0)\n\n        # Get prediction\n        with torch.no_grad():\n            logits, _ = model(img_tensor.unsqueeze(1))  # Add seq dim\n            pred_class = logits.argmax(dim=1).item()\n\n        # Generate Grad-CAM for predicted class\n        cam = gradcam(img_tensor, class_idx=pred_class)\n\n        # Original\n        axes[row, 0].imshow(img_np)\n        axes[row, 0].set_title(f\"True: {class_names[true_label]}\\nPred: {class_names[pred_class]}\")\n        axes[row, 0].axis('off')\n\n        # Heatmap\n        axes[row, 1].imshow(cam, cmap='jet')\n        axes[row, 1].set_title(\"Grad-CAM\")\n        axes[row, 1].axis('off')\n\n        # Overlay\n        overlay = np.uint8(255 * img_np * 0.6 + cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET) * 0.4)\n        axes[row, 2].imshow(cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB))\n        axes[row, 2].set_title(\"Overlay\")\n        axes[row, 2].axis('off')\n\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T18:07:26.556929Z","iopub.execute_input":"2025-12-04T18:07:26.557204Z","iopub.status.idle":"2025-12-04T18:07:26.569632Z","shell.execute_reply.started":"2025-12-04T18:07:26.557184Z","shell.execute_reply":"2025-12-04T18:07:26.568585Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"\n\n# # ---------------------------\n# # Visualization with Grad-CAM\n# # ---------------------------\n# def visualize_gradcam(model, dataloader, device, gradcam, num_samples=3):\n#     model.eval()\n#     count = 0\n#     for imgs, labels, paths in dataloader:\n#         if count >= num_samples:\n#             break\n#         imgs, labels = imgs.to(device), labels.to(device)\n#         img_np = imgs[0].cpu().numpy().transpose(1, 2, 0)\n#         img_tensor = imgs[0:1].unsqueeze(1)\n\n#         with torch.no_grad():\n#             logits, _ = model(img_tensor)\n#             pred_class = logits.argmax(dim=1).item()\n\n#         cam = gradcam(img_tensor.squeeze(1), class_idx=pred_class)\n\n#         plt.figure(figsize=(12, 4))\n#         plt.subplot(1, 3, 1)\n#         plt.imshow(img_np)\n#         plt.title(f\"Original\\nTrue: {Config.classes[labels[0]]}\\nPred: {Config.classes[pred_class]}\")\n#         plt.axis('off')\n\n#         plt.subplot(1, 3, 2)\n#         plt.imshow(cam, cmap='jet')\n#         plt.title(\"Grad-CAM\")\n#         plt.axis('off')\n\n#         overlay = np.uint8(255 * img_np * 0.6 + cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET) * 0.4)\n#         plt.subplot(1, 3, 3)\n#         plt.imshow(cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB))\n#         plt.title(\"Overlay\")\n#         plt.axis('off')\n\n#         plt.tight_layout()\n#         plt.show()\n#         count += 1\n\n\nif __name__ == \"__main__\":\n    DATA_ROOT = \"/kaggle/input/foggy-cityscapes-image-dataset/Foggy_Cityscapes\"  \n\n    # Step 1: Collect all samples\n    all_samples = []\n    class_to_idx = {cls: idx for idx, cls in enumerate(Config.classes)}\n\n    for class_name in Config.classes:\n        class_dir = os.path.join(DATA_ROOT, class_name)\n        if not os.path.exists(class_dir):\n            raise FileNotFoundError(f\"Directory not found: {class_dir}\")\n        for fname in os.listdir(class_dir):\n            if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n                all_samples.append((os.path.join(class_dir, fname), class_to_idx[class_name]))\n\n    print(f\"‚úÖ Total images loaded: {len(all_samples)}\")\n\n    # Step 2: Stratified split (80% train, 20% val)\n    image_paths, labels = zip(*all_samples)\n    train_paths, val_paths, train_labels, val_labels = train_test_split(\n        image_paths, labels, test_size=0.2, random_state=42, stratify=labels\n    )\n\n    train_samples = list(zip(train_paths, train_labels))\n    val_samples = list(zip(val_paths, val_labels))\n\n    print(f\"SplitOptions ‚Üí Train: {len(train_samples)}, Val: {len(val_samples)}\")\n    print(f\"Train class counts: {np.bincount(train_labels)}\")\n    print(f\"Val class counts: {np.bincount(val_labels)}\")\n\n    # Step 3: Transforms\n    transform = transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.Resize((Config.img_size, Config.img_size)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n    # Step 4: Datasets & Loaders\n    train_ds = FogClassificationDataset(train_samples, transform=transform)\n    val_ds = FogClassificationDataset(val_samples, transform=transform)\n\n    train_loader = DataLoader(train_ds, batch_size=Config.batch_size, shuffle=True, num_workers=Config.num_workers)\n    val_loader = DataLoader(val_ds, batch_size=Config.batch_size, shuffle=False, num_workers=Config.num_workers)\n\n    # Step 5: Model + Training\n    model = FogClassifier(num_classes=Config.num_classes, use_ltc=True).to(Config.device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=Config.lr, weight_decay=1e-4)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=Config.epochs)\n\n    gradcam = GradCAM(model, model.backbone_layer4)\n\n    best_acc = 0.0\n    for epoch in range(Config.epochs):\n        tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, Config.device)\n        val_loss, val_acc = validate(model, val_loader, Config.device)\n        scheduler.step()\n\n        print(f\"Epoch {epoch+1}/{Config.epochs} - Train Loss: {tr_loss:.4f}, Acc: {tr_acc:.4f} | Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}\")\n\n        if val_acc > best_acc:\n            best_acc = val_acc\n            torch.save(model.state_dict(), os.path.join(Config.SAVE_DIR, \"best_fog_classifier.pth\"))\n            print(\"‚úÖ Saved best model!\")\n\n    # # Step 6: Visualize\n    # print(\"\\nüîç Generating Grad-CAM visualizations...\")\n    # visualize_gradcam(model, val_loader, Config.device, gradcam, num_samples=3)\n    # gradcam.remove_hook()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T18:07:26.573712Z","iopub.execute_input":"2025-12-04T18:07:26.573998Z","execution_failed":"2025-12-04T19:37:03.481Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Total images loaded: 1500\nSplitOptions ‚Üí Train: 1200, Val: 300\nTrain class counts: [400 400 400]\nVal class counts: [100 100 100]\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/75 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1830: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\nTraining: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75/75 [06:26<00:00,  5.15s/it, Loss=0.364, Acc=0.709]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10 - Train Loss: 0.6720, Acc: 0.7092 | Val Loss: 1.1688, Acc: 0.4833\n‚úÖ Saved best model!\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75/75 [06:21<00:00,  5.08s/it, Loss=0.349, Acc=0.802]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/10 - Train Loss: 0.4813, Acc: 0.8025 | Val Loss: 0.8121, Acc: 0.6667\n‚úÖ Saved best model!\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75/75 [06:17<00:00,  5.03s/it, Loss=0.371, Acc=0.839]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/10 - Train Loss: 0.4171, Acc: 0.8392 | Val Loss: 0.3592, Acc: 0.8700\n‚úÖ Saved best model!\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75/75 [06:19<00:00,  5.06s/it, Loss=0.305, Acc=0.838]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/10 - Train Loss: 0.4172, Acc: 0.8375 | Val Loss: 0.6963, Acc: 0.6967\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75/75 [06:18<00:00,  5.05s/it, Loss=0.277, Acc=0.877] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/10 - Train Loss: 0.3007, Acc: 0.8775 | Val Loss: 0.2306, Acc: 0.9267\n‚úÖ Saved best model!\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75/75 [06:25<00:00,  5.15s/it, Loss=0.272, Acc=0.902] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/10 - Train Loss: 0.2670, Acc: 0.9025 | Val Loss: 0.2297, Acc: 0.9267\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75/75 [06:20<00:00,  5.07s/it, Loss=0.0479, Acc=0.952]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/10 - Train Loss: 0.1391, Acc: 0.9517 | Val Loss: 0.1853, Acc: 0.9300\n‚úÖ Saved best model!\n","output_type":"stream"},{"name":"stderr","text":"Training:  15%|‚ñà‚ñç        | 11/75 [01:01<05:27,  5.12s/it, Loss=0.0458, Acc=0.932]","output_type":"stream"}],"execution_count":null}]}